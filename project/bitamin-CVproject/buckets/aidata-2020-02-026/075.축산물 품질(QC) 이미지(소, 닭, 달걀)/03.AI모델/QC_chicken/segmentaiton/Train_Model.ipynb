{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "\n",
    "# Pytorch 10.2 GPU install\n",
    "#!pip install torch\n",
    "\n",
    "# Pytorch 10.2 CPU install\n",
    "#!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "#!pip install torchvision\n",
    "#!pip install torchinfo\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU 코어 개수를 확인\n",
    "import os\n",
    "n_cores = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from Model import UNet\n",
    "from Dataset import Dataset, ToTensor, Normalization, RandomFlip\n",
    "from Utils import IOU_Numpy\n",
    "from EarlyStopping import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치 패키지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('python version:',sys.version)\n",
    "print('numpy version:', np.__version__)\n",
    "print('torch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 파라미터 (Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 1e-3\n",
    "# 배치 사이즈\n",
    "batch_size = 24\n",
    "# Iteration 수\n",
    "num_epoch = 100\n",
    "\n",
    "# 데이터를 읽어올 Path\n",
    "data_dir = \"../Dataset/preprocessed/segmentation/\"\n",
    "# 모델을 저장할 Path\n",
    "ckpt_dir = \"./Models/\"\n",
    "\n",
    "# 모델 이름\n",
    "Model_name = \"chicken_segmentation\"\n",
    "# 모델을 연산할 장비 설정 (가능하면 GPU, 안되면 CPU에서 계산함)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Each Set\n",
      "Train set: 23975, Validation set: 3304, Test set: 3304\n"
     ]
    }
   ],
   "source": [
    "# Test 데이터를 읽어옴\n",
    "test_dataset = Dataset(data_dir=data_dir, train=False)\n",
    "# 갯수를 저장함\n",
    "n_test = len(test_dataset)\n",
    "\n",
    "# transform 적용해서 Train 데이터 셋 불러오기\n",
    "train_transform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
    "train_dataset = Dataset(data_dir=data_dir, train=True, transform=train_transform)\n",
    "# 전체 Train 데이터 셋 개수를 저장\n",
    "n_whole_train = len(train_dataset)\n",
    "\n",
    "# Train 개수를 Test 갯수만큼 Train와 Vlidation set으루 나눔\n",
    "train_ds, valid_ds = torch.utils.data.random_split(train_dataset, [int(n_whole_train - n_test), n_test])\n",
    "\n",
    "# Data Loader를 구성함\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, num_workers=int(n_cores/2), pin_memory=False)\n",
    "valid_loader = DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=True, num_workers=int(n_cores/2), pin_memory=False)\n",
    "\n",
    "print(\"The Number of Each Set\")\n",
    "print('Train set: %d, Validation set: %d, Test set: %d' %(len(train_ds), len(valid_ds), n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs are available!!\n",
      "DataParallel(\n",
      "  (module): UNet(\n",
      "    (enc1_1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (enc1_2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (enc2_1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (enc2_2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (enc3_1): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (enc3_2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (enc4_1): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (enc4_2): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (enc5_1): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (dec5_1): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (unpool4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (dec4_2): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (dec4_1): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (unpool3): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (dec3_2): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (dec3_1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (unpool2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (dec2_2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (dec2_1): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (unpool1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (dec1_2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (dec1_1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (fc): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=15, verbose=True, path=ckpt_dir)\n",
    "\n",
    "# 네트워크 불러오기\n",
    "net = UNet().to(device) # device : cpu or gpu\n",
    "\n",
    "# 모델을 여러 GPU에 돌리기\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(torch.cuda.device_count(), \"GPUs are available!!\")\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "# loss 정의\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# Optimizer 정의\n",
    "optim = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "optimizer_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optim,\n",
    "                                                     mode=\"min\",\n",
    "                                                     factor=0.5,\n",
    "                                                     patience=5,\n",
    "                                                     verbose=True)\n",
    "\n",
    "# 모델 확인\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:01<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001 / 100 | train loss 0.1218 | valid loss 0.0959 | vallid iou 0.8942\n",
      "Validation loss decreased (inf --> 0.105816).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:46<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 002 / 100 | train loss 0.0924 | valid loss 0.0896 | vallid iou 0.9053\n",
      "Validation loss decreased (0.105816 --> 0.094731).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:52<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 003 / 100 | train loss 0.0874 | valid loss 0.0860 | vallid iou 0.9041\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:43<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 004 / 100 | train loss 0.0855 | valid loss 0.0848 | vallid iou 0.9054\n",
      "Validation loss decreased (0.094731 --> 0.094648).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:46<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 005 / 100 | train loss 0.0843 | valid loss 0.0833 | vallid iou 0.9065\n",
      "Validation loss decreased (0.094648 --> 0.093459).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:49<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 006 / 100 | train loss 0.0831 | valid loss 0.0824 | vallid iou 0.9102\n",
      "Validation loss decreased (0.093459 --> 0.089800).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:53<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 007 / 100 | train loss 0.0824 | valid loss 0.0819 | vallid iou 0.9089\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:48<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 008 / 100 | train loss 0.0817 | valid loss 0.0817 | vallid iou 0.9112\n",
      "Validation loss decreased (0.089800 --> 0.088850).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:58<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 009 / 100 | train loss 0.0812 | valid loss 0.0806 | vallid iou 0.9082\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:45<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 010 / 100 | train loss 0.0805 | valid loss 0.0813 | vallid iou 0.9105\n",
      "EarlyStopping counter: 2 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:53<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 011 / 100 | train loss 0.0801 | valid loss 0.0884 | vallid iou 0.9004\n",
      "EarlyStopping counter: 3 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:51<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 012 / 100 | train loss 0.0797 | valid loss 0.0797 | vallid iou 0.9107\n",
      "EarlyStopping counter: 4 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:03<00:00,  1.51s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 013 / 100 | train loss 0.0793 | valid loss 0.0792 | vallid iou 0.9085\n",
      "EarlyStopping counter: 5 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:59<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 014 / 100 | train loss 0.0789 | valid loss 0.0797 | vallid iou 0.9089\n",
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n",
      "EarlyStopping counter: 6 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:50<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 015 / 100 | train loss 0.0774 | valid loss 0.0789 | vallid iou 0.9116\n",
      "Validation loss decreased (0.088850 --> 0.088392).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:58<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 016 / 100 | train loss 0.0771 | valid loss 0.0785 | vallid iou 0.9115\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:56<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 017 / 100 | train loss 0.0768 | valid loss 0.0784 | vallid iou 0.9118\n",
      "Validation loss decreased (0.088392 --> 0.088226).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:56<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 018 / 100 | train loss 0.0766 | valid loss 0.0783 | vallid iou 0.9112\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:53<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 019 / 100 | train loss 0.0763 | valid loss 0.0783 | vallid iou 0.9110\n",
      "EarlyStopping counter: 2 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:00<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 020 / 100 | train loss 0.0761 | valid loss 0.0787 | vallid iou 0.9107\n",
      "EarlyStopping counter: 3 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:53<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 021 / 100 | train loss 0.0759 | valid loss 0.0784 | vallid iou 0.9114\n",
      "EarlyStopping counter: 4 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:49<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 022 / 100 | train loss 0.0756 | valid loss 0.0783 | vallid iou 0.9114\n",
      "EarlyStopping counter: 5 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:54<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 023 / 100 | train loss 0.0754 | valid loss 0.0785 | vallid iou 0.9135\n",
      "Validation loss decreased (0.088226 --> 0.086460).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:57<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 024 / 100 | train loss 0.0751 | valid loss 0.0785 | vallid iou 0.9114\n",
      "EarlyStopping counter: 1 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:55<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 025 / 100 | train loss 0.0749 | valid loss 0.0785 | vallid iou 0.9121\n",
      "EarlyStopping counter: 2 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:50<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 026 / 100 | train loss 0.0746 | valid loss 0.0791 | vallid iou 0.9133\n",
      "EarlyStopping counter: 3 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:52<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 027 / 100 | train loss 0.0744 | valid loss 0.0790 | vallid iou 0.9134\n",
      "EarlyStopping counter: 4 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:54<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 028 / 100 | train loss 0.0739 | valid loss 0.0790 | vallid iou 0.9114\n",
      "EarlyStopping counter: 5 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:05<00:00,  1.51s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 029 / 100 | train loss 0.0737 | valid loss 0.0794 | vallid iou 0.9131\n",
      "Epoch    29: reducing learning rate of group 0 to 2.5000e-04.\n",
      "EarlyStopping counter: 6 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:08<00:00,  1.51s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 030 / 100 | train loss 0.0724 | valid loss 0.0793 | vallid iou 0.9118\n",
      "EarlyStopping counter: 7 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:54<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 031 / 100 | train loss 0.0719 | valid loss 0.0798 | vallid iou 0.9120\n",
      "EarlyStopping counter: 8 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:56<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 032 / 100 | train loss 0.0715 | valid loss 0.0803 | vallid iou 0.9115\n",
      "EarlyStopping counter: 9 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:04<00:00,  1.51s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 033 / 100 | train loss 0.0713 | valid loss 0.0798 | vallid iou 0.9118\n",
      "EarlyStopping counter: 10 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:07<00:00,  1.51s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 034 / 100 | train loss 0.0708 | valid loss 0.0803 | vallid iou 0.9116\n",
      "EarlyStopping counter: 11 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:56<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 035 / 100 | train loss 0.0705 | valid loss 0.0812 | vallid iou 0.9123\n",
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-04.\n",
      "EarlyStopping counter: 12 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:52<00:00,  1.49s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 036 / 100 | train loss 0.0695 | valid loss 0.0810 | vallid iou 0.9108\n",
      "EarlyStopping counter: 13 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [24:57<00:00,  1.50s/it]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 037 / 100 | train loss 0.0692 | valid loss 0.0817 | vallid iou 0.9108\n",
      "EarlyStopping counter: 14 out of 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [25:02<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 038 / 100 | train loss 0.0688 | valid loss 0.0823 | vallid iou 0.9116\n",
      "EarlyStopping counter: 15 out of 15\n",
      "All works done!!!\n"
     ]
    }
   ],
   "source": [
    "# Automatic Mixed-Precision(AMP)를 위한 Gradscaler 선언\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True) \n",
    "\n",
    "# Iteration 수 만큼 학습 시킴\n",
    "for epoch in range(num_epoch):\n",
    "    net.train()\n",
    "    train_loss_arr = []\n",
    "    \n",
    "    # Train set을 Batch size 만큼 가져옴\n",
    "    for data in tqdm(train_loader):\n",
    "        # forward\n",
    "        inputs = data['input'].to(device) # 데이터 device로 올리기\n",
    "        label = data['label'].to(device) \n",
    "        \n",
    "        # Casts operations to mixed precision \n",
    "        with torch.cuda.amp.autocast():     \n",
    "            output = net(inputs)\n",
    "            train_loss = fn_loss(output, label)  # output과 label 사이의 loss 계산\n",
    "        \n",
    "        scaler.scale(train_loss).backward()  # gradient backpropagation with AMP\n",
    "        scaler.step(optim)  # backpropa 된 gradient를 이용해서 각 layer의 parameters update with AMP\n",
    "        \n",
    "        # Updates the scale for next iteration \n",
    "        scaler.update()\n",
    "        \n",
    "        # backward\n",
    "        optim.zero_grad()  # gradient 초기화\n",
    "        \n",
    "        # loss를 저장함\n",
    "        train_loss_arr += [train_loss.item()]\n",
    "    \n",
    "    # Validation을 진행하여 Early stopping을 적용함\n",
    "    with torch.no_grad():  # validation 이기 때문에 backpropa 진행 x, 학습된 네트워크가 정답과 얼마나 가까운지 loss만 계산\n",
    "        net.eval()  # 네트워크를 evaluation 용으로 선언\n",
    "        val_loss_arr = []\n",
    "        val_iou_arr = []\n",
    "        \n",
    "        for batch, data in enumerate(valid_loader):\n",
    "            # forward\n",
    "            inputs = data['input'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "            output = net(inputs)\n",
    "            \n",
    "            # loss\n",
    "            val_loss = fn_loss(output, label)\n",
    "            val_loss_arr += [val_loss.item()]\n",
    "            val_iou_arr += [IOU_Numpy(fn_classifier(output), label)]\n",
    "    \n",
    "    # Validation이 끝나면 Iteration 후 정보를 출력\n",
    "    print('epoch %03d / %03d | train loss %.4f | valid loss %.4f | vallid iou %.4f' % (\n",
    "            epoch+1, num_epoch, np.mean(train_loss_arr), np.mean(val_loss_arr), np.mean(val_iou_arr)))  \n",
    "    \n",
    "    # Optimizaer를 Update함\n",
    "    optimizer_scheduler.step(1 - np.mean(val_iou_arr))\n",
    "    \n",
    "    # Validation IOU에 따라 Early stopping을 판단함\n",
    "    models_dict = dict()\n",
    "    models_dict[Model_name] = net.state_dict()\n",
    "    early_stopping(1 - np.mean(val_iou_arr), models_dict)\n",
    "\n",
    "    # Early stopping patient가 초과되면 멈춤\n",
    "    if early_stopping.early_stop:\n",
    "        break\n",
    "\n",
    "print(\"All works done!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
